{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mohsin \n",
    "#Assignment 13 NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_train = pd.read_csv('salarytrainnb.csv')\n",
    "salarydata_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_test = pd.read_csv('salarytestnb.csv')\n",
    "salarydata_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 30161 instances and 14 attributes in the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 15060 instances and 14 attributes in the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View top 5 rows of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the Training dataset\n",
    "\n",
    "salarydata_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the Test dataset\n",
    "\n",
    "salarydata_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View summary of Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salarydata_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the special characters in the data frame \n",
    "salarydata_train.isin(['?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the special characters in the data frame \n",
    "salarydata_test.isin(['?']).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salarydata_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [var for var in salarydata_train.columns if salarydata_train[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
    "\n",
    "print('The categorical variables are :\\n\\n', categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the categorical variables\n",
    "\n",
    "salarydata_train[categorical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of categorical variables\n",
    "\n",
    "There are 9 categorical variables.\n",
    "\n",
    "The categorical variables are given by workclass, education, maritalstatus, occupation, relationship, race, sex, native and Salary.\n",
    "\n",
    "Salary is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore problems within categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables\n",
    "salarydata_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are no missing values in the categorical variables. I will confirm this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency counts of values in categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    \n",
    "    print(salarydata_train[var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency distribution of categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    \n",
    "    print(salarydata_train[var].value_counts()/np.float(len(salarydata_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in workclass variable\n",
    "\n",
    "salarydata_train.workclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in workclass variable\n",
    "\n",
    "salarydata_train.workclass.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore occupation variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in occupation variable\n",
    "\n",
    "salarydata_train.occupation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in occupation variable\n",
    "\n",
    "salarydata_train.occupation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore native_country variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in native_country variable\n",
    "\n",
    "salarydata_train.native.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in native_country variable\n",
    "\n",
    "salarydata_train.native.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of labels: cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for cardinality in categorical variables\n",
    "\n",
    "for var in categorical:\n",
    "    \n",
    "    print(var, ' contains ', len(salarydata_train[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find numerical variables\n",
    "\n",
    "numerical = [var for var in salarydata_train.columns if salarydata_train[var].dtype!='O']\n",
    "\n",
    "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
    "\n",
    "print('The numerical variables are :', numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the numerical variables\n",
    "\n",
    "salarydata_train[numerical].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of numerical variables\n",
    "\n",
    "There are 5 numerical variables.\n",
    "\n",
    "These are given by age, educationno, capitalgain, capitalloss and hoursperweek.\n",
    "All of the numerical variables are of discrete data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore problems within numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables\n",
    "\n",
    "salarydata_train[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare feature vector and target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = salarydata_train.drop(['Salary'], axis=1)\n",
    "\n",
    "y = salarydata_train['Salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into separate training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display categorical variables\n",
    "\n",
    "categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display numerical variables\n",
    "\n",
    "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
    "\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the categorical variables in training set\n",
    "\n",
    "X_train[categorical].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables with missing data\n",
    "\n",
    "for col in categorical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, (X_train[col].isnull().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical variables with most frequent value\n",
    "\n",
    "for df2 in [X_train, X_test]:\n",
    "    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n",
    "    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n",
    "    df2['native'].fillna(X_train['native'].mode()[0], inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_train\n",
    "\n",
    "X_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_test\n",
    "\n",
    "X_test[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_train\n",
    "\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_test\n",
    "\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables\n",
    "\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category encoders\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode remaining variables with one-hot encoding\n",
    "\n",
    "encoder = ce.OneHotEncoder(cols=['workclass', 'education', 'maritalstatus', 'occupation', 'relationship', \n",
    "                                 'race', 'sex', 'native'])\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that from the initial 14 columns, we now have 102 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have training and testing set ready for model building. \n",
    "Before that, we should map all the feature variables onto the same scale. \n",
    "It is called feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have X_train dataset ready to be fed into the Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, y_test are the true class labels and y_pred are the predicted class labels in the test-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the train-set and test-set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = gnb.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training-set accuracy score is 0.8023 while the test-set accuracy to be 0.7995. \n",
    "These two values are quite comparable. So, there is no sign of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare model accuracy with null accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check class distribution in test set\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null accuracy score\n",
    "\n",
    "null_accuracy = (7407/(7407+2362))\n",
    "\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model accuracy score is 0.8023 but null accuracy score is 0.7582. \n",
    "So, we can conclude that our Gaussian Naive Bayes Classification model is doing a very good job in predicting the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
    "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification accuracy\n",
    "\n",
    "classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classification error\n",
    "\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print('Classification error : {0:0.4f}'.format(classification_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision score\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Positive Rate\n",
    "\n",
    "True Positive Rate is synonymous with Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_rate = TP / float(TP + FN)\n",
    "\n",
    "\n",
    "print('True Positive Rate : {0:0.4f}'.format(true_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate class probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of two classes- 0 and 1\n",
    "\n",
    "y_pred_prob = gnb.predict_proba(X_test)[0:10]\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "* In each row, the numbers sum to 1.\n",
    "\n",
    "* There are 2 columns which correspond to 2 classes - <=50K and >50K.\n",
    "\n",
    "        * Class 0 => <=50K - Class that a person makes less than equal to 50K.\n",
    "\n",
    "        * Class 1 => >50K - Class that a person makes more than 50K.\n",
    "\n",
    "* Importance of predicted probabilities\n",
    "\n",
    "    * We can rank the observations by probability of whether a person makes less than or equal to 50K or more than 50K.\n",
    "\n",
    "* predict_proba process\n",
    "\n",
    "    * Predicts the probabilities\n",
    "\n",
    "    * Choose the class with the highest probability\n",
    "\n",
    "* Classification threshold level\n",
    "\n",
    "    * There is a classification threshold level of 0.5.\n",
    "\n",
    "    * Class 0 => <=50K - probability of salary less than or equal to 50K is predicted if probability < 0.5.\n",
    "\n",
    "    * Class 1 => >50K - probability of salary more than 50K is predicted if probability > 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the probabilities in dataframe\n",
    "\n",
    "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - <=50K', 'Prob of - >50K'])\n",
    "\n",
    "y_pred_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "gnb.predict_proba(X_test)[0:10, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "y_pred1 = gnb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of predicted probabilities\n",
    "\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# plot histogram with 10 bins\n",
    "plt.hist(y_pred1, bins = 10)\n",
    "\n",
    "\n",
    "# set the title of predicted probabilities\n",
    "plt.title('Histogram of predicted probabilities of salaries >50K')\n",
    "\n",
    "\n",
    "# set the x-axis limit\n",
    "plt.xlim(0,1)\n",
    "\n",
    "\n",
    "# set the title\n",
    "plt.xlabel('Predicted probabilities of salaries >50K')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC - AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred1, pos_label = '>50K')\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.title('ROC curve for Gaussian Naive Bayes Classifier for Predicting Salaries')\n",
    "\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute ROC AUC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "ROC_AUC = roc_auc_score(y_test, y_pred1)\n",
    "\n",
    "print('ROC AUC : {:.4f}'.format(ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cross-validated ROC AUC \n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Cross_validated_ROC_AUC = cross_val_score(gnb, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "print('Cross validated ROC AUC : {:.4f}'.format(Cross_validated_ROC_AUC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying 10-Fold Cross Validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(gnb, X_train, y_train, cv = 10, scoring='accuracy')\n",
    "\n",
    "print('Cross-validation scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Average cross-validation score\n",
    "\n",
    "print('Average cross-validation score: {:.4f}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Assignment 12 - Naive Bayes.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
